<!DOCTYPE html>
<html lang="en-GB">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" href="/favicon.ico" />
    <title>Blog post for CSC1028 | mck.is</title>
    <meta name="description" content="A summary of the project" />
    <meta http-equiv="X-Clacks-Overhead" content="GNU Terry Pratchett" />

    <link rel="stylesheet" href="/assets/css/simple.min.css">
    <!--<link rel="stylesheet" href="/C:/Users/James/Desktop/Website/james-mck.github.io/assets/css/simple.css">-->

    <meta http-equiv="Content-Security-Policy" content="default-src 'none'; font-src 'self'; img-src 'self'; script-src 'self'; style-src 'self';">
  </head>


  <body>

    <header>
      <h1>ðŸ’» James' Blog</h1>
      <p>Searching for a tagline.</p>

      <nav>
        <a href="/">Home</a>
        <a href="/about">About</a>
        <a href="/blog">Blog</a>
        <a href="/projects">Projects</a>
        <a href="/about#contact">Contact</a>
      </nav>
    </header>

    <main>
<h1 id="title">Blog post for CSC1028</h1>
<em>A summary of the project</em><br/>
<small>Published on Fri 04 Mar 2022. 1167 words.</small><br/>
<small>Tags: <a href="/blog/tags/nodejs">nodejs</a> | <a href="/blog/tags/project%20sonar">project sonar</a> | <a href="/blog/tags/programming">programming</a></small>
<hr/>
<h1 id="url-understanding-tool"> URL Understanding Tool</h1>
<p>This project was created over the course of 7 weeks for my CSC1028 module, and although it is not a fully complete project, it provides a great framework for future work.</p>

<h3 id="aims-of-the-project"> Aims of the project</h3>
<p>This project aims to be a tool for cybersecurity or power users to provide as much relevant metadata on a given URL as possible. Although there are only currently a few sources of data, the application is set up to be as easy as possible to add sources to.</p>

<p>The project has 3 main parts:</p>

<h2 id="http-apis"> HTTP APIs</h2>
<p><a href="https://github.com/James-McK/CSC1028APIs">GitHub</a>  <br/>
The main component of this project is a set of HTTP APIs that can be queried for information on a URL/IP address to provide information from various sources, from local databases to external APIs.  The current data sources are:<ul>
 	<li>A local MongoDB database containing data from Project Sonar</li>
 	<li>A local MongoDB database containing data on phishing/malware URLs from <a href="https://phishtank.org/">Phishtank</a>, <a href="https://openphish.com/">OpenPhish</a>, <a href="https://urlhaus.abuse.ch/">URLHaus</a> and <a href="https://malwarediscoverer.com/">MalwareDiscoverer</a>.</li>
 	<li>Earliest page/hostname archive date, from <a href="https://archive.org">https://archive.org</a>.</li>
 	<li><a href="https://www.similarweb.com/">Similarweb</a> global website rank</li>
 	<li>IP Geolocation data (Currently from <a href="https://ip-api.com/">https://ip-api.com/</a>, could probably be improved - this section did not have much thought put into it, and was mostly done as a proof of concept)</li>
</ul>

</p>

<p>Several of these can also be queried via the command line, i.e. <code>node queryArchiveDate.js example.com</code></p>

<p>For more information on dealing with Project Sonar's data, see <a href="https://mck.is/project-sonar/">my how-to guide</a>, but in summary, the data is stored in a local MongoDB database which, when full, can fill up to 60gb. We then use <a href="https://docs.mongodb.com/manual/core/index-text/">text indexes</a> to allow <em>extremely</em> performant queries to be made.</p>

<p>Note on Project Sonar's data: 6 days after I wrote my how-to guide, <a href="https://www.rapid7.com/blog/post/2022/02/10/evolving-how-we-share-rapid7-research-data-2/">Rapid7 switched to requiring you to apply</a> to access Project Sonar's data. Except now, a few weeks later, it no longer requires an account again, and this time I cannot find any blog post etc. mentioning this change back, so I do not know if this is a permanent or temporary change.</p>

<h3 id="retrieving-data"> Retrieving data</h3>
<p>To retrieve the data used for the above HTTP APIs, some of the modules send a request to an external API, while some query a local MongoDB database. To fetch the data used to fill up the MongoDB database, there exists two programs: One for parsing and inserting Project Sonar's data, and one for fetching, parsing and inserting malware/phishing data.</p>

<h3 id="creating-the-http-apis"> Creating the HTTP APIs</h3>
<p>To create and manage the HTTP APIs, there is a single program (<code>createAllAPI.js</code>) that opens up all the APIs when run (Ports 10130 to 10135 by default). This program does almost nothing itself, and imports functionality from other modules to create the APIs (Notably <code>createHTTPServer.js</code>, which will take any function and open up an API for it on the given port.). This approach allows new APIs to be added with ease, and allows you to manage which modules are started.</p>

<h3 id="running/developing-the-application"> Running/developing the application</h3>
<p>For developing any of this project, you'll need a few things set up and installed. I'd recommend following the setup process I used in <a href="https://mck.is/project-sonar/#setup">my how-to guide</a>. You'll also want to install the dependancies listed in <code>package.json</code> with <code>npm install &#60;package_name&#62;</code>. To actually get the data, you'll first want to run <code>./fetch/fetchMalwarePhishingData.js</code> and <code>./fetch/fetchMalwarePhishingData.js</code> (Assuming you've downloaded Project Sonar's data in a similar way as I did in my <a href="https://mck.is/project-sonar/#parsing-a-local-copy-of-project-sonar">how-to guide</a>).  <br/>
You can then run <code>node ./create/createAllAPI.js</code> to start the APIs.</p>

<h2 id="electron-app"> Electron App</h2>
<p><img src="ElectronUI2.png" alt="Electron app UI" loading="lazy" /> <a href="https://github.com/James-McK/CSC1028ElectronApp">GitHub</a></p>

<p>The electron app provides a user-friendly interface allowing the user to make queries regarding any URL, and displays the data to the user in a better format than the entirely raw JSON, however further steps should be taken as the current presentation is still not easily readable.</p>

<p>Since it is built with electron, the page is little more than a HTML page with some javascript behind it! As a result, all this app has to do is query the back-end HTTP APIs and display the result to the user!</p>

<h3 id="running-the-application"> Running the application</h3>
<p>Assuming you've followed the steps above for running/developing the central node.js app (Which you should have done, as this electron app isn't too useful without it), not much more is required to run the electron app. After opening the folder, you'll need to run <code>npm install --save-dev electron</code> to install everything required for electron. You can then run <code>npm start</code> to start the app.  <br/>
You might also want to look at <a href="https://www.electronjs.org/docs/latest/tutorial/quick-start/">https://www.electronjs.org/docs/latest/tutorial/quick-start/</a> for an introduction to Electron.</p>

<h2 id="browser-addon"> Browser Addon</h2>
<p><img src="BasicAddon.png" alt="Basic Addon UI" loading="lazy" /> <a href="https://github.com/James-McK/CSC1028FFAddon">GitHub</a></p>

<p>The browser addon is extremely similar to the electron app, providing a user-friendly front end to the data, built with HTML and javascript. As it is integrated into the browser, it can automatically fetch and cache data as the user navigates the web.  <br/>
Note: The addon currently only supports Firefox, however it could be ported to support Chromium-based browsers extremely easily, as both share an extremely similar base API, with only a few functions being located in different namespaces, but providing the same results. (See <a href="https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons/WebExtensions/Chrome_incompatibilities">Chrome incompatibilities</a> on MDN for details)</p>

<p>The addon's UI is also currently lacking as I chose to shift focus away from it, as I decided the Electron UI was more important initially. However, since both are based on HTML and javascript, and the Electron app was built upon the framework of the browser addon, the updates for the Electron app should be able to be ported without too much effort.</p>

<h3 id="installing-the-addon"> Installing the addon</h3>
<p>(Currently Firefox-only)  <br/>
Installing the addon is thankfully easy. Navigate to <code>about:debugging</code> and click on the "This Firefox" tab. Click on "Load Temporary Add-on..." and navigate to the folder containing the addon files. Click on any of the files (e.g. <code>manifest.json</code>) and load it. The addon is now loaded! Whenever you update your code and save it, you just need to click the "Reload" button that appears.  <br/>
I'd also reccommend looking at <a href="https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons/WebExtensions">MDN</a> for excellent doccumentation of the WebExtension APIs.</p>

<p><img src="LoadingAddon.png" alt="Loading the addon" loading="lazy" /></p>

<h2 id="improvements-and-vision"> Improvements and vision</h2>
<p>The project in its current state is nowhere near complete, but serves as a foundation to build further upon.</p>

<p>There are many possible new data sources that could be integrated into the project, for example:<ul>
 	<li>Crowdsourced datasets, eg Trustpilot and other user-driven sources of metadata</li>
 	<li>What tech stack companies are using, and alert the user to suspicious activity if a different result is actually found, using information from <a href="https://stackshare.io">https://stackshare.io</a></li>
 	<li>Further integration with archives, e.g. thumbnails of pages from the <a href="https://web.archive.org/">Wayback Machine</a> - If a webpage has only existed for a few days its chance of being malware or a phishing attack are higher</li>
 	<li>Data from <a href="https://commoncrawl.org/">Common Crawl</a> to find sites that point to a given page (They were having <a href="https://groups.google.com/g/common-crawl/c/kEHzXZNu5To">issues with 503 errors</a> when I last looked into integrating this, although it appears to have been fixed since.)</li>
 	<li>Possibly other sources of data like Mozilla Observatory or Google Lighthouse.</li>
 	<li>General improvements to the user experience.</li>
</ul>

  And many other possible sources of interesting metadata!</p>

    </main>

    <footer>
      <p><small>
        This page was handmade by James McKee using <code>&lt;\&gt;</code> and <a href="https://simplecss.org">Simple.css</a>, and is currently hosted on GitHub Pages.<br/><br/>
        <a href="https://512kb.club"><img src="/assets/images/green-team.svg" alt="Green team member of the 512kb club"/></a><br/>
      </small></p>
    </footer>
  </body>
</html>
